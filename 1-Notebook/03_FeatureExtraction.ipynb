{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chosen_gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e75a62662976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchosen_gpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_freer_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_freer_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Configured device: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chosen_gpu'"
     ]
    }
   ],
   "source": [
    "from chosen_gpu import get_freer_gpu\n",
    "device = torch.device(get_freer_gpu()) \n",
    "print(\"Configured device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/np/par1/par1_com2_data_2021-03-27-13.52.44_visual_1_X.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b3d0ef8336b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/np/{par}/{file}{task}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/np/{par}/{file}{task}_X.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/np/{par}/{file}{task}_y.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/np/par1/par1_com2_data_2021-03-27-13.52.44_visual_1_X.npy'"
     ]
    }
   ],
   "source": [
    "par = \"par1\"\n",
    "file = \"par1_com2_data_2021-03-27-13.52.44\"\n",
    "task = \"_visual_1\"\n",
    "\n",
    "path = 'data/np/{par}/{file}{task}.npy'.format(par=par,file=file, task=task)\n",
    "\n",
    "X = np.load('data/np/{par}/{file}{task}_X.npy'.format(par=par,file=file, task=task))\n",
    "y = np.load('data/np/{par}/{file}{task}_y.npy'.format(par=par,file=file, task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [# stim, # electrod, # datapoint]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot to see wheter eegs have drift or not\n",
    "data = X.reshape(-1,16)\n",
    "print(data.shape)\n",
    "fig, ax = plt.subplots(16,1,figsize=(200,100),sharex=True)\n",
    "\n",
    "start_point = 3000\n",
    "plot_lenght = 3200\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    ax[i].plot(range(plot_lenght),data[start_point:start_point+plot_lenght,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Chunking \n",
    "- For X : Chunking eeg to lengh of 10 data point in each stimuli's eeg\n",
    "- For y(lebels) : Filled the lebels in y because we chunk X ( 1 stimuli into 6 chunk). We have 500 labels before but now we need 500 x 6 = 3000 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 10 \n",
    "# print(f'Shape of X : {X.shape}')\n",
    "# print(f'Shape of y : {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# def chunk_data(data, size):\n",
    "#     data_keep = data.shape[2] - (data.shape[2]%size)\n",
    "#     #print(f'{data.shape}')\n",
    "#     data      = data[:,:,:data_keep]\n",
    "#     #print(f'{data.shape}')\n",
    "#     #print(data[0,0,:20])\n",
    "#     data      = data.reshape(-1,data.shape[1],data.shape[2]//size,size)\n",
    "#     #print(f'{data.shape}')\n",
    "#     #print(data[0,0,:2,:])\n",
    "#     data      = np.transpose(data, (0, 2, 1, 3)  )\n",
    "#     #print(f'{data.shape}')\n",
    "#     #print(data[0,:2,0,:])\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# def filled_y(y   , chunk_num):\n",
    "#     yy = np.array(  [ [ i ] *chunk_num for i in  y     ] ).ravel()\n",
    "\n",
    "#     return yy\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Oringinal X shape {X.shape}')\n",
    "# X_chunk = chunk_data(X, chunk_size)\n",
    "# X       = X_chunk\n",
    "# print(f'Reshape X to : {X.shape}')\n",
    "# chunk_per_stim = X.shape[1]\n",
    "# X       = X.reshape(-1,16,10)\n",
    "# print(f'Reshpae X to : {X.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Shape of y : {y.shape}')\n",
    "# y   = filled_y(y , chunk_per_stim)\n",
    "# print(f'Shape of new y : {y.shape}')\n",
    "# print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "### 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEncoder(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, height , width)\n",
    "                          (64   , 16      , 1      , 63   )\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(    nn.Conv1d(16, 32, kernel_size=(1,3),   padding=(0,0), stride=(1,1))  ,  self.activation )\n",
    "        self.conv2 = nn.Sequential(    nn.Conv1d(32, 64, kernel_size=(1,3) ,  padding=(0,0), stride=(1,1))  ,  self.activation )\n",
    "        self.fc1   = nn.Sequential(    nn.Linear(7808,256),  self.activation ,nn.Dropout(0.1)   ,nn.BatchNorm1d(256)   )\n",
    "        self.fc2   = nn.Sequential(    nn.Linear(256,128),  self.activation ,nn.Dropout(0.1)   ,nn.BatchNorm1d(128) )\n",
    "        self.fc3   = nn.Sequential(    nn.Linear(128,64),  self.activation  ,nn.Dropout(0.1)   ,nn.BatchNorm1d(64) )\n",
    "        self.fc4   = nn.Sequential(    nn.Linear(64,32),  self.activation   ,nn.Dropout(0.1)   ,nn.BatchNorm1d(32) )\n",
    "        self.fc5   = nn.Sequential(    nn.Linear(32,10)   )\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.is_debug= False\n",
    "        \n",
    "    def encode(self, X):\n",
    "        \n",
    "        \n",
    "        if self.is_debug  : print('--------Convolute--------'); print(X.shape) \n",
    "            \n",
    "        X = self.conv1(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "            \n",
    "        X = self.conv2(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "            \n",
    "        X = X.flatten(start_dim = 1)\n",
    "\n",
    "        # print(X.shape) \n",
    " \n",
    "        X = self.fc1(X)\n",
    "        if self.is_debug : print('--------Flatten--------') ; print(X.shape) \n",
    "\n",
    "        X = self.fc2(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "\n",
    "        X = self.fc3(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "        \n",
    "        X = self.fc4(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "\n",
    "        X = self.fc5(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "\n",
    "            \n",
    "        return X\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.encode(X)\n",
    "        return X\n",
    "    \n",
    "    def get_latent( self, X):\n",
    "        if self.is_debug  : print('--------Convolute--------'); print(X.shape) \n",
    "            \n",
    "        X = self.conv1(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "            \n",
    "        X = self.conv2(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "            \n",
    "        X = X.flatten(start_dim = 1)\n",
    "        if self.is_debug  : print('--------Flatten--------') ; print(X.shape) \n",
    " \n",
    "        X = self.fc1(X)\n",
    "        if self.is_debug : print('--------Flatten--------') ; print(X.shape) \n",
    "\n",
    "        X = self.fc2(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "\n",
    "        X = self.fc3(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "        \n",
    "        X = self.fc4(X)\n",
    "        if self.is_debug  : print(X.shape) \n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def classifier(self, latent):\n",
    "        return self.fc5(latent)\n",
    "    \n",
    "# torch_X = torch.from_numpy(X)\n",
    "# torch_X1 = torch_X.reshape(torch_X.shape[0],torch_X.shape[1],1,torch_X.shape[2]).float()\n",
    "# model_AE = AE()\n",
    "# model_AE.is_debug = True\n",
    "# model_AE.forward(torch_X1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "### 5. Define Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training process\n",
    "\n",
    "We set `model.train()` so dropout is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predicted_list = []\n",
    "    model.train()\n",
    "    \n",
    "    for batch, labels in iterator:\n",
    "        \n",
    "        #Move tensors to the configured device\n",
    "        batch  = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "       \n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(batch.float())\n",
    "        outputs = outputs.to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels).to(device)\n",
    "\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        #check accuracy\n",
    "        predictions = model(batch.float())\n",
    "        _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "        total += labels.size(0)  #keep track of total\n",
    "        correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "        acc = 100 * (correct / total)\n",
    "                \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc = acc\n",
    "        predicted_list.append(predicted)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc, predicted_list\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predicted_list = []\n",
    "    labels_list    = []\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch, labels in iterator:\n",
    "            \n",
    "            #Move tensors to the configured device\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #print(labels)\n",
    "            \n",
    "\n",
    "            predictions = model(batch.float())\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            \n",
    "            _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "            #print(predicted)\n",
    "            \n",
    "            total += labels.size(0)  #keep track of total\n",
    "            correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "            acc = 100 * (correct / total)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "            \n",
    "            labels_list.append(labels)\n",
    "            predicted_list.append(predicted)\n",
    "           \n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) ,predicted_list, labels_list\n",
    "\n",
    "\n",
    "# define a time function useful for calculating time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "\n",
    "def do_plot(train_losses, valid_losses):\n",
    "    plt.figure(figsize=(25,5))\n",
    "#     clear_output(wait=True)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.title('Train and Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "### 6. Actual Training for Feature Extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Define model parameters\n",
    "- Count model parameters\n",
    "- optimizer\n",
    "- loss function\n",
    "- GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model from class\n",
    "model_EEGEncoder = EEGEncoder()\n",
    "model_EEGEncoder = model_EEGEncoder.float() #define precision as float to reduce running time\n",
    "models   = [model_EEGEncoder]\n",
    "\n",
    "\n",
    "#Count the parameters for writing papers\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "for model in models:\n",
    "    print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Prepare X and y in correct shape\n",
    "\n",
    "- For X, pytorch (if set batch_first) LSTM requires to be (batch, seq_len, features).  Thus, for us, it should be (100, 75, 16).\n",
    "- For y, nothing is special\n",
    "- So let's convert our numpy to pytorch, and then reshape using view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X = torch.from_numpy(X)\n",
    "torch_y = torch.from_numpy(y)\n",
    "\n",
    "torch_X = torch_X\n",
    "\n",
    "\n",
    "print(torch_X.shape)\n",
    "print(torch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original X: \", torch_X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN requires the input shape as (batch, channel, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_reshaped = torch_X.reshape(torch_X.shape[0],torch_X.shape[1],1,torch_X.shape[2])\n",
    "print(\"Converted X to \", torch_X_reshaped.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Split test train set, and load them into a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Define dataset\n",
    "ds = TensorDataset(torch_X_reshaped, torch_y)\n",
    "\n",
    "#Train test split\n",
    "train_size = int(torch_X_reshaped.size()[0] * 0.7)\n",
    "valid_size = int(torch_X_reshaped.size()[0] * 0.2)\n",
    "test_size  = torch_X_reshaped.size()[0] - train_size - valid_size\n",
    "\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(ds, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128 #keeping it binary so it fits GPU\n",
    "#Train set loader\n",
    "train_iterator = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "#Validation set loader\n",
    "valid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)\n",
    "\n",
    "#Test set loader\n",
    "test_iterator = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                          batch_size=test_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from chosen_gpu import get_freer_gpu\n",
    "device = torch.device(get_freer_gpu()) \n",
    "print(\"Configured device: \", device)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses    = []\n",
    "valid_losses    = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "N_EPOCHS      = 1000          ## best is 10k\n",
    "criterion     = nn.CrossEntropyLoss()\n",
    "optimizer     = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_debug=False\n",
    "iteration = 0\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Training {type(model).__name__}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_predicted    = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc, valid_predicted, _ = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        iteration     += 1\n",
    "        \n",
    "        if (epoch+1) % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'Epoch: {epoch+1:02}/{N_EPOCHS}  |',end='')\n",
    "            print(f'\\tTrain Loss: {train_loss:.5f}  | Train Acc: {train_acc:.2f}%  |', end='')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.5f}  | Val. Acc: {valid_acc:.2f}%')\n",
    "            do_plot(train_losses, valid_losses)\n",
    "            \n",
    "                    #--------- Display loss---------\n",
    "#         if batch_i % print_every == 0:\n",
    "#             clear_output(wait=True)\n",
    "#             print('Epoch : {:1d}/{:1d} | Total iteration : {:1d} | d_loss: {:6.6f} | g_loss: {:6.6f}'.format(epoch+1, NUM_EPOCH, iteration, d_loss.item(), g_loss.item()))\n",
    "            \n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            print(\"Model:{} saved.\".format(type(model).__name__))\n",
    "\n",
    "            if epoch%200 == 0 :\n",
    "                torch.save(model.state_dict(), \"../weights/HCI/feature_extract/{par}{model}{epoch}.pt.tar\".format(par=par,model=type(model).__name__,epoch=epoch))\n",
    "            \n",
    "            best_model_index = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation (Test model)\n",
    "using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_to_list(_tmp):\n",
    "    from functools import reduce\n",
    "    import operator\n",
    "\n",
    "    xx     = [ i.cpu().detach().numpy().ravel().tolist() for i in _tmp]\n",
    "    xx     = reduce(operator.concat, xx)\n",
    "    return xx\n",
    "\n",
    "\n",
    "\n",
    "models[best_model_index].load_state_dict(torch.load('../weights/HCI/feature_extract/par1EEGEncoder0.pt.tar'))\n",
    "\n",
    "\n",
    "\n",
    "# test_loss = evaluate(models[best_model_index], test_iterator, criterion)\n",
    "# print(f'Test Loss: {test_loss}') # | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "test_loss, test_acc , predicted, actual_labels = evaluate(models[best_model_index], test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')\n",
    "print(\"---------------\")\n",
    "print(\" (Actual y , Predicted y)\")\n",
    "\n",
    "y_test     = squeeze_to_list(actual_labels)\n",
    "y_hat_test = squeeze_to_list(predicted)\n",
    "\n",
    "out = zip(y_test, y_hat_test)\n",
    "\n",
    "print(list(out))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save features extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder_best_model = models[best_model_index].load_state_dict(torch.load(f'save/{type(model).__name__}{i}.pth.tar'))\n",
    "# torch.save(encoder_best_model, 'save/7.1_encoder_best_model.pth.tar')\n",
    "\n",
    "\n",
    "### save Encoder network\n",
    "# torch.save(model_EEGEncoder, 'save/model_EEGEncoder_network_5s.pt.tar')\n",
    "# torch.save(test_iterator,'save/eeg_X_test_5s.pt.tar')\n",
    "\n",
    "\n",
    "# save extracted features\n",
    "eeg_encode = model_EEGEncoder.get_latent(torch_X_reshaped.to(device).float())\n",
    "eeg_extracted_features = eeg_encode.detach().cpu().numpy()\n",
    "np.save('data/extracted_features/{par}/{file}{task}_X'.format(par=par,file=file, task=task), eeg_extracted_features )\n",
    "np.save('data/extracted_features/{par}/{file}{task}_y'.format(par=par,file=file, task=task),y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
